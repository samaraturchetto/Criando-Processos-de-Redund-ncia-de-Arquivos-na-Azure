# üè≠ Criando Processos de Redund√¢ncia de Arquivos na Azure com Data Factory

Este reposit√≥rio faz parte do Desafio de Projeto da DIO **"Criando Processos de Redund√¢ncia de Arquivos na Azure"**, do Bootcamp **"Microsoft AI for Tech - Azure Databricks"**.

---

## üìò Introdu√ß√£o

Este projeto tem como objetivo criar um pipeline completo de redund√¢ncia de arquivos utilizando recursos do **Microsoft Azure**. A proposta √© guiar o usu√°rio a configurar toda a infraestrutura necess√°ria com **Azure Data Factory**, **SQL Server** (local e na nuvem) e **Azure Data Lake (Blob Storage)**, movimentando dados entre esses ambientes, criando arquivos `.txt` organizados por camadas de dados em containers (`bronze`, `prata`, `ouro`).

---

## ‚öôÔ∏è Cria√ß√£o de Processos de Redund√¢ncia de Arquivos na Azure ‚Äì Data Factory

### 1. Configura√ß√£o do Ambiente

#### 1.1 Criar Recursos no Azure
- Azure Data Factory (ADF)  
- Azure SQL Database  
- Azure Blob Storage  
  - Criar containers: `bronze`, `prata`, `ouro`

#### 1.2 Preparar o Ambiente On-Premises (local)
- Ter o SQL Server local com banco e tabelas dispon√≠veis  
- Verificar acesso administrativo ao servidor para configurar o Integration Runtime  

---

### 2. Configurar Integration Runtime Self-hosted

- No Azure Data Factory, v√° em **Manage > Integration Runtimes > New**
- Escolha **Self-hosted** e siga o assistente:
  - Baixe o instalador (modo express ou manual)
  - Instale no servidor local (on-premises)
  - Configure com a chave gerada pelo ADF
  - Verifique se o status est√° como ‚ÄúRunning‚Äù

---

### 3. Criar Linked Services

#### 3.1 Para o SQL Server On-Premises
- V√° em **Manage > Linked Services > New > SQL Server**
- Utilize o Integration Runtime criado
- Insira servidor, banco, autentica√ß√£o e teste a conex√£o

#### 3.2 Para o Azure SQL Database
- Criar outro Linked Service apontando para o banco na nuvem

#### 3.3 Para o Azure Blob Storage
- Criar um Linked Service usando a Storage Account
- Inserir a chave de acesso da conta

---

### 4. Criar Datasets

#### 4.1 Dataset de Origem
- Criar um dataset do tipo **SQL Server Table**
- Escolher o Linked Service do SQL Server on-premises
- Definir a tabela que ser√° copiada

#### 4.2 Dataset de Destino
- Criar um dataset do tipo **DelimitedText (.txt)**
- Escolher o Linked Service do Blob Storage
- Apontar para o container/caminho desejado (`bronze`, `prata`, `ouro`)
- Configurar delimitador e nome do arquivo

---

### 5. Criar Pipeline de C√≥pia

#### 5.1 Construir o Pipeline
- No Data Factory, ir para **Author > Pipelines > New pipeline**
- Adicionar a atividade **Copy Data**
  - **Source (origem):** Dataset do SQL Server local
  - **Sink (destino):** Dataset no Blob Storage
- Nomear e salvar

#### 5.2 Validar e Publicar
- Clique em **Validate** para checar erros
- Clique em **Publish All** para aplicar mudan√ßas

---

### 6. Executar e Monitorar

#### 6.1 Executar Pipeline
- Rodar o pipeline manualmente (**Debug**) ou configurar um trigger (**Trigger Now**)

#### 6.2 Verificar Resultados
- V√° at√© o container do Azure Blob Storage
- Verifique se o arquivo `.txt` foi gerado com sucesso

#### 6.3 Analisar Performance
- No painel do Data Factory, abra a aba **Monitor**
- Verifique dura√ß√£o da execu√ß√£o, throughput, logs e outras informa√ß√µes

---

## üí° Boas Pr√°ticas

- Configurar o **Integration Runtime** corretamente antes de usar Linked Services on-premises  
- Sempre testar conex√µes antes de criar datasets  
- Organizar os containers do Blob Storage em camadas (`bronze`, `silver`, `gold`)  
- Validar e publicar mudan√ßas antes de executar  
- Documentar e salvar as chaves do IR  
- Monitorar performance para controle de custos  

---

## üõ† Tecnologias Utilizadas

- Microsoft Azure (ADF, SQL, Blob Storage)  
- SQL Server  
- Data Factory Integration Runtime  
- Azure Portal  
